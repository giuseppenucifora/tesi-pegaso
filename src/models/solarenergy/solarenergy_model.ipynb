{
 "cells": [
  {
   "cell_type": "code",
   "id": "8adcbe0819b88578",
   "metadata": {},
   "source": [
    "'''\n",
    "from opt_einsum.paths import branch_1\n",
    "!apt-get update\n",
    "!apt-get install graphviz -y\n",
    "\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "\n",
    "!pip install keras\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install joblib\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install scipy\n",
    "!pip install seaborn\n",
    "!pip install tqdm\n",
    "!pip install pydot\n",
    "!pip install tensorflow-io\n",
    "!pip install tensorflow-addons\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a813e3cbca057b7",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, MultiHeadAttention, Dropout, BatchNormalization, LayerNormalization, Input, Activation, Lambda, Bidirectional, Add, MaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "folder_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "random_state_value = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3f525e19f78a1da",
   "metadata": {},
   "source": [
    "def get_season(date):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    if (month == 12 and day >= 21) or (month <= 3 and day < 20):\n",
    "        return 'Winter'\n",
    "    elif (month == 3 and day >= 20) or (month <= 6 and day < 21):\n",
    "        return 'Spring'\n",
    "    elif (month == 6 and day >= 21) or (month <= 9 and day < 23):\n",
    "        return 'Summer'\n",
    "    elif (month == 9 and day >= 23) or (month <= 12 and day < 21):\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "def get_time_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "\n",
    "def add_time_features(df):\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['timestamp'] = df['datetime'].astype(np.int64) // 10 ** 9\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "    df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "    df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "    df['week_of_year'] = df['datetime'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['datetime'].dt.quarter\n",
    "    df['is_month_end'] = df['datetime'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_end'] = df['datetime'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_end'] = df['datetime'].dt.is_year_end.astype(int)\n",
    "    df['month_sin'] = np.sin(df['month'] * (2 * np.pi / 12))\n",
    "    df['month_cos'] = np.cos(df['month'] * (2 * np.pi / 12))\n",
    "    df['day_of_year_sin'] = np.sin(df['day_of_year'] * (2 * np.pi / 365.25))\n",
    "    df['day_of_year_cos'] = np.cos(df['day_of_year'] * (2 * np.pi / 365.25))\n",
    "    df['season'] = df['datetime'].apply(get_season)\n",
    "    df['time_period'] = df['hour'].apply(get_time_period)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_solar_features(df):\n",
    "    # Calcolo dell'angolo solare\n",
    "    df['solar_angle'] = np.sin(df['day_of_year'] * (2 * np.pi / 365.25)) * np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "\n",
    "    # Interazioni tra features rilevanti\n",
    "    df['cloud_temp_interaction'] = df['cloudcover'] * df['temp']\n",
    "    df['visibility_cloud_interaction'] = df['visibility'] * (100 - df['cloudcover'])\n",
    "\n",
    "    # Feature derivate\n",
    "    df['clear_sky_index'] = (100 - df['cloudcover']) / 100\n",
    "    df['temp_gradient'] = df['temp'] - df['tempmin']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_solar_specific_features(df):\n",
    "    # Angolo solare e durata del giorno\n",
    "    df['day_length'] = 12 + 3 * np.sin(2 * np.pi * (df['day_of_year'] - 81) / 365.25)\n",
    "    df['solar_noon'] = 12 - df['hour']\n",
    "    df['solar_elevation'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25) * np.cos(2 * np.pi * df['solar_noon'] / 24)\n",
    "\n",
    "    # Interazioni\n",
    "    df['cloud_elevation'] = df['cloudcover'] * df['solar_elevation']\n",
    "    df['visibility_elevation'] = df['visibility'] * df['solar_elevation']\n",
    "\n",
    "    # Rolling features con finestre più ampie\n",
    "    df['cloud_rolling_12h'] = df['cloudcover'].rolling(window=12).mean()\n",
    "    df['temp_rolling_12h'] = df['temp'].rolling(window=12).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_radiation_energy_features(df):\n",
    "    \"\"\"Aggiunge feature specifiche basate su solarenergy e uvindex\"\"\"\n",
    "\n",
    "    # Rapporto tra energia solare e UV (indipendente da solarradiation)\n",
    "    df['energy_uv_ratio'] = df['solarenergy'] / (df['uvindex'] + 1e-6)\n",
    "\n",
    "    # Aggregazioni temporali\n",
    "    # Medie mobili\n",
    "    windows = [3, 6, 12, 24]  # ore\n",
    "    for w in windows:\n",
    "        df[f'energy_rolling_mean_{w}h'] = df['solarenergy'].rolling(window=w).mean()\n",
    "        df[f'uv_rolling_mean_{w}h'] = df['uvindex'].rolling(window=w).mean()\n",
    "\n",
    "    # Aggregazioni giornaliere\n",
    "    df['energy_daily_sum'] = df.groupby(df.index.date)['solarenergy'].transform('sum')\n",
    "    df['uv_daily_max'] = df.groupby(df.index.date)['uvindex'].transform('max')\n",
    "\n",
    "    # Variazioni\n",
    "    df['energy_change'] = df['solarenergy'].diff()\n",
    "    df['uv_change'] = df['uvindex'].diff()\n",
    "\n",
    "    # Lag features\n",
    "    lags = [1, 2, 3, 6, 12, 24]  # ore\n",
    "    for lag in lags:\n",
    "        df[f'energy_lag_{lag}h'] = df['solarenergy'].shift(lag)\n",
    "        df[f'uv_lag_{lag}h'] = df['uvindex'].shift(lag)\n",
    "\n",
    "    # Indicatori di picco\n",
    "    df['is_energy_peak'] = (df['solarenergy'] > df['energy_rolling_mean_6h'] * 1.2).astype(int)\n",
    "    df['is_uv_peak'] = (df['uvindex'] > df['uv_rolling_mean_6h'] * 1.2).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_advanced_features(df):\n",
    "    # Features esistenti\n",
    "    df = add_time_features(df)\n",
    "    df = add_solar_features(df)\n",
    "    df = add_solar_specific_features(df)\n",
    "    df = add_radiation_energy_features(df)\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # One-hot encoding per le feature categoriche\n",
    "    df = pd.get_dummies(df, columns=['season', 'time_period'])\n",
    "\n",
    "    # Interazioni tra variabili meteorologiche\n",
    "    df['temp_humidity'] = df['temp'] * df['humidity']\n",
    "    df['temp_cloudcover'] = df['temp'] * df['cloudcover']\n",
    "    df['visibility_cloudcover'] = df['visibility'] * df['cloudcover']\n",
    "\n",
    "    # Features derivate per la radiazione solare\n",
    "    df['clear_sky_factor'] = (100 - df['cloudcover']) / 100\n",
    "    df['day_length'] = np.sin(df['day_of_year_sin']) * 12 + 12  # approssimazione della durata del giorno\n",
    "\n",
    "    # Lag features\n",
    "    df['temp_1h_lag'] = df['temp'].shift(1)\n",
    "    df['cloudcover_1h_lag'] = df['cloudcover'].shift(1)\n",
    "    df['humidity_1h_lag'] = df['humidity'].shift(1)\n",
    "\n",
    "    # Rolling means\n",
    "    df['temp_rolling_mean_6h'] = df['temp'].rolling(window=6).mean()\n",
    "    df['cloudcover_rolling_mean_6h'] = df['cloudcover'].rolling(window=6).mean()\n",
    "\n",
    "    df['temp_humidity_interaction'] = df['temp'] * df['humidity'] / 100\n",
    "\n",
    "    # Indicatore di condizioni estreme\n",
    "    df['extreme_conditions'] = ((df['temp'] > df['temp'].quantile(0.75)) & (df['humidity'] < df['humidity'].quantile(0.25))).astype(int)\n",
    "\n",
    "    # Feature composite per la trasparenza atmosferica\n",
    "    df['atmospheric_transparency'] = (100 - df['cloudcover']) * (df['visibility'] / 10)\n",
    "\n",
    "    # Indicatori temporali più granulari per mezze stagioni\n",
    "    df['is_transition_season'] = ((df['season_Spring'] | df['season_Autumn'])).astype(int)\n",
    "\n",
    "    # Interazione tra angolo solare e copertura nuvolosa normalizzata\n",
    "    df['solar_cloud_effect'] = df['solar_elevation'] * (100 - df['cloudcover']) / 100\n",
    "\n",
    "    # Indicatore di stabilità atmosferica\n",
    "    df['pressure_stability'] = df.groupby(df.index.date if isinstance(df.index, pd.DatetimeIndex)\n",
    "                                          else df.index.to_series().dt.date)['pressure'].transform(\n",
    "        lambda x: x.std()\n",
    "    ).fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_advanced_data(df):\n",
    "    # Applicazione delle funzioni di feature engineering\n",
    "    df = add_advanced_features(df)\n",
    "\n",
    "    target_variables = ['solarradiation', 'solarenergy', 'uvindex']\n",
    "\n",
    "    # Feature selection aggiornata (senza dipendenze da solarradiation)\n",
    "    selected_features = [\n",
    "        # Features meteorologiche base\n",
    "        'temp', 'humidity', 'cloudcover', 'visibility', 'pressure',\n",
    "\n",
    "        # Features solari\n",
    "        'zenith_angle', 'air_mass', 'atmospheric_transmission',\n",
    "        'cloud_transmission', 'theoretical_radiation',\n",
    "\n",
    "        # Features temporali\n",
    "        'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "        'day_of_year_sin', 'day_of_year_cos',\n",
    "\n",
    "        # Features atmosferiche\n",
    "        'clear_sky_index', 'humidity_factor', 'atmospheric_clarity',\n",
    "        'vapor_pressure',\n",
    "\n",
    "        # Feature energia solare e UV\n",
    "        'energy_uv_ratio',\n",
    "\n",
    "        # Medie mobili\n",
    "        'energy_rolling_mean_3h', 'energy_rolling_mean_6h',\n",
    "        'uv_rolling_mean_3h', 'uv_rolling_mean_6h',\n",
    "\n",
    "        # Aggregazioni giornaliere\n",
    "        'energy_daily_sum', 'uv_daily_max',\n",
    "\n",
    "        # Lag features principali\n",
    "        'energy_lag_1h', 'energy_lag_3h', 'energy_lag_6h',\n",
    "        'uv_lag_1h', 'uv_lag_3h',\n",
    "\n",
    "        # Indicatori di picco e volatilità\n",
    "        'is_energy_peak', 'is_uv_peak',\n",
    "        'energy_volatility', 'uv_volatility',\n",
    "\n",
    "        # Indici compositi\n",
    "        'solar_intensity_index',\n",
    "\n",
    "        # Interazioni\n",
    "        'uv_cloud_interaction',\n",
    "        'energy_temp_interaction'\n",
    "    ]\n",
    "\n",
    "    # Aggiungi colonne one-hot\n",
    "    categorical_columns = [col for col in df.columns if col.startswith(('season_', 'time_period_'))]\n",
    "    final_features = selected_features + categorical_columns\n",
    "\n",
    "    # Preparazione del dataset\n",
    "    df = df.sort_values('datetime')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # Gestione valori mancanti\n",
    "    for column in final_features + target_variables:\n",
    "        df[column] = df[column].interpolate(method='time')\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Split temporale\n",
    "    data_after_2010 = df[df['year'] >= 2010].copy()\n",
    "    data_before_2010 = df[df['year'] < 2010].copy()\n",
    "\n",
    "    X = data_after_2010[final_features]\n",
    "    y = data_after_2010['solarradiation']\n",
    "    X_to_predict = data_before_2010[final_features]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=random_state_value)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_to_predict_scaled = scaler.transform(X_to_predict)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, final_features, X_to_predict_scaled\n",
    "\n",
    "\n",
    "def create_sequence_data(X, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Converte i dati in sequenze per l'input LSTM\n",
    "    sequence_length rappresenta quante ore precedenti considerare\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        sequences.append(X[i:i + sequence_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "def prepare_hybrid_data(df):\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, scaler, features, X_to_predict_scaled = prepare_advanced_data(df)\n",
    "\n",
    "    # Convertiamo i dati in sequenze\n",
    "    sequence_length = 24  # 24 ore di dati storici\n",
    "\n",
    "    X_train_seq = create_sequence_data(X_train_scaled, sequence_length)\n",
    "    X_test_seq = create_sequence_data(X_test_scaled, sequence_length)\n",
    "\n",
    "    # Adattiamo le y rimuovendo i primi (sequence_length-1) elementi\n",
    "    y_train = y_train[sequence_length - 1:]\n",
    "    y_test = y_test[sequence_length - 1:]\n",
    "\n",
    "    X_to_predict_seq = create_sequence_data(X_to_predict_scaled, sequence_length)\n",
    "\n",
    "    return X_train_seq, X_test_seq, y_train, y_test, scaler, features, X_to_predict_seq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dff3259-b376-4cfc-89d8-ab2ea18aaa5e",
   "metadata": {},
   "source": [
    "def create_residual_lstm_layer(x, units, dropout_rate, l2_reg=0.01, return_sequences=True, survival_probability=0.8):\n",
    "    residual = x\n",
    "    x = Bidirectional(LSTM(units, return_sequences=return_sequences, kernel_regularizer=regularizers.l2(l2_reg)))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    if return_sequences:\n",
    "        if int(residual.shape[-1]) != 2 * units:\n",
    "            residual = Dense(2 * units, activation='linear')(residual)\n",
    "        x = tfa.layers.StochasticDepth(survival_probability)([x, residual])\n",
    "    return x\n",
    "\n",
    "def attention_block(x, units, num_heads=8, survival_probability=0.8):\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
    "    x = tfa.layers.StochasticDepth(survival_probability)([x, attention])\n",
    "    x = LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def create_solarradiation_model(input_shape, folder_name, l2_lambda=0.005):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Progressive survival probabilities\n",
    "    survival_probs = [0.9, 0.8, 0.7]\n",
    "    attention_survival_probs = [0.85, 0.75, 0.65]\n",
    "\n",
    "    # Progressive units for LSTM layers\n",
    "    lstm_units = [256, 128, 64]\n",
    "\n",
    "    # Progressive dropout rates\n",
    "    dropout_rates = [0.4, 0.3, 0.2]\n",
    "\n",
    "    # Number of attention heads for each block\n",
    "    attention_heads = [32, 24, 16]\n",
    "\n",
    "    # LSTM layers with attention blocks\n",
    "    x = inputs\n",
    "    for i in range(3):\n",
    "        x = create_residual_lstm_layer(\n",
    "            x,\n",
    "            units=lstm_units[i],\n",
    "            dropout_rate=dropout_rates[i],\n",
    "            l2_reg=l2_lambda,\n",
    "            return_sequences=True,\n",
    "            survival_probability=survival_probs[i]\n",
    "        )\n",
    "        x = attention_block(\n",
    "            x,\n",
    "            units=lstm_units[i],\n",
    "            num_heads=attention_heads[i],\n",
    "            survival_probability=attention_survival_probs[i]\n",
    "        )\n",
    "        if i < 2:  # No pooling after last LSTM layer\n",
    "            x = MaxPooling1D()(x)\n",
    "\n",
    "    # Final LSTM layer without return sequences\n",
    "    x = create_residual_lstm_layer(\n",
    "        x,\n",
    "        units=32,\n",
    "        dropout_rate=0.1,\n",
    "        l2_reg=l2_lambda,\n",
    "        return_sequences=False,\n",
    "        survival_probability=0.6  # Lowest survival probability for final layer\n",
    "    )\n",
    "\n",
    "    # Dense layers with progressive narrowing\n",
    "    dense_units = [64, 32]\n",
    "    dense_dropout = [0.2, 0.1]\n",
    "\n",
    "    for units, dropout in zip(dense_units, dense_dropout):\n",
    "        x = Dense(units, kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    outputs = Lambda(lambda x: tf.clip_by_value(x, 0, 1500))(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"SolarRadiationModel\")\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=0.0003,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "\n",
    "    # Custom metrics\n",
    "    def rmse(y_true, y_pred):\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "    def mape(y_true, y_pred):\n",
    "        epsilon = 1e-7\n",
    "        return tf.reduce_mean(tf.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "    # Hybrid loss combining MSE and MAE\n",
    "    def hybrid_loss(y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "        return 0.7 * mse + 0.3 * mae\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=hybrid_loss,\n",
    "        metrics=[\n",
    "            'mae',\n",
    "            rmse,\n",
    "            mape\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    plot_model(model,\n",
    "               to_file=f'{folder_name}_model_architecture.png',\n",
    "               show_shapes=True,\n",
    "               show_layer_names=True,\n",
    "               dpi=150,\n",
    "               show_layer_activations=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_solarradiation_predictions(y_true, y_pred, folder_name=None):\n",
    "    \"\"\"\n",
    "    Valutazione specifica per la radiazione solare con metriche appropriate\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Valori reali della radiazione solare (W/m²)\n",
    "    y_pred : array-like\n",
    "        Valori predetti della radiazione solare (W/m²)\n",
    "    folder_name : str, optional\n",
    "        Cartella dove salvare eventuali plot di analisi\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dizionario contenente tutte le metriche calcolate\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "\n",
    "    # Calcolo metriche sui valori raw\n",
    "    mae_raw = mean_absolute_error(y_true, y_pred)\n",
    "    rmse_raw = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2_raw = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Calcolo MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-7))) * 100\n",
    "\n",
    "    # Calcolo accuratezza per diversi margini di errore percentuale\n",
    "    within_5_percent = np.mean(np.abs((y_pred - y_true) / (y_true + 1e-7)) <= 0.05)\n",
    "    within_10_percent = np.mean(np.abs((y_pred - y_true) / (y_true + 1e-7)) <= 0.10)\n",
    "    within_20_percent = np.mean(np.abs((y_pred - y_true) / (y_true + 1e-7)) <= 0.20)\n",
    "\n",
    "    # Definizione dei livelli di radiazione solare (W/m²)\n",
    "    def get_radiation_level(value):\n",
    "        if value <= 200:\n",
    "            return 'Very Low'\n",
    "        elif value <= 400:\n",
    "            return 'Low'\n",
    "        elif value <= 600:\n",
    "            return 'Moderate'\n",
    "        elif value <= 800:\n",
    "            return 'High'\n",
    "        elif value <= 1000:\n",
    "            return 'Very High'\n",
    "        else:\n",
    "            return 'Extreme'\n",
    "\n",
    "    # Calcola livelli di radiazione\n",
    "    y_true_levels = [get_radiation_level(v) for v in y_true]\n",
    "    y_pred_levels = [get_radiation_level(v) for v in y_pred]\n",
    "\n",
    "    # Calcola accuracy dei livelli\n",
    "    level_accuracy = np.mean([t == p for t, p in zip(y_true_levels, y_pred_levels)])\n",
    "\n",
    "    print(\"\\nSolar Radiation Prediction Metrics:\")\n",
    "    print(\"\\nAbsolute Metrics:\")\n",
    "    print(f\"MAE: {mae_raw:.2f} W/m²\")\n",
    "    print(f\"RMSE: {rmse_raw:.2f} W/m²\")\n",
    "    print(f\"R² Score: {r2_raw:.3f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "    print(\"\\nPercentage-based Accuracy:\")\n",
    "    print(f\"Within ±5%: {within_5_percent:.3f}\")\n",
    "    print(f\"Within ±10%: {within_10_percent:.3f}\")\n",
    "    print(f\"Within ±20%: {within_20_percent:.3f}\")\n",
    "\n",
    "    print(\"\\nRadiation Level Accuracy:\")\n",
    "    print(f\"Level Accuracy: {level_accuracy:.3f}\")\n",
    "\n",
    "    print(\"\\nRadiation Level Confusion Matrix:\")\n",
    "    print(pd.crosstab(\n",
    "        pd.Series(y_true_levels, name='Actual'),\n",
    "        pd.Series(y_pred_levels, name='Predicted')\n",
    "    ))\n",
    "\n",
    "    # Analisi degli errori per diverse fasce orarie\n",
    "    if 'hour' in locals():\n",
    "        day_periods = {\n",
    "            'Morning (5-11)': (5, 11),\n",
    "            'Noon (11-13)': (11, 13),\n",
    "            'Afternoon (13-17)': (13, 17),\n",
    "            'Evening (17-21)': (17, 21),\n",
    "            'Night (21-5)': (21, 5)\n",
    "        }\n",
    "\n",
    "        print(\"\\nError Analysis by Time of Day:\")\n",
    "        for period, (start, end) in day_periods.items():\n",
    "            if start < end:\n",
    "                mask = (hour >= start) & (hour < end)\n",
    "            else:  # Per gestire il periodo notturno\n",
    "                mask = (hour >= start) | (hour < end)\n",
    "\n",
    "            period_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "            period_mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + 1e-7))) * 100\n",
    "            print(f\"\\n{period}:\")\n",
    "            print(f\"MAE: {period_mae:.2f} W/m²\")\n",
    "            print(f\"MAPE: {period_mape:.2f}%\")\n",
    "\n",
    "    # Se specificata una cartella, salva i plot di analisi\n",
    "    if folder_name is not None:\n",
    "        try:\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            plt.figure(figsize=(15, 10))\n",
    "\n",
    "            # Plot 1: Scatter plot\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "            plt.plot([0, max(y_true.max(), y_pred.max())],\n",
    "                     [0, max(y_true.max(), y_pred.max())],\n",
    "                     'r--', lw=2)\n",
    "            plt.xlabel('Actual Solar Radiation (W/m²)')\n",
    "            plt.ylabel('Predicted Solar Radiation (W/m²)')\n",
    "            plt.title('Predicted vs Actual Values')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Plot 2: Distribuzione errori assoluti\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.hist(y_pred - y_true, bins=50, alpha=0.7)\n",
    "            plt.xlabel('Prediction Error (W/m²)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Absolute Errors')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Plot 3: Distribuzione errori percentuali\n",
    "            plt.subplot(2, 2, 3)\n",
    "            percentage_errors = ((y_pred - y_true) / (y_true + 1e-7)) * 100\n",
    "            plt.hist(np.clip(percentage_errors, -100, 100), bins=50, alpha=0.7)\n",
    "            plt.xlabel('Percentage Error (%)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Percentage Errors')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Plot 4: Box plot degli errori per livello di radiazione\n",
    "            plt.subplot(2, 2, 4)\n",
    "            errors_by_level = pd.DataFrame({\n",
    "                'Level': y_true_levels,\n",
    "                'Error': y_pred - y_true\n",
    "            })\n",
    "            errors_by_level.boxplot(column='Error', by='Level', figsize=(10, 6))\n",
    "            plt.xlabel('Actual Radiation Level')\n",
    "            plt.ylabel('Prediction Error (W/m²)')\n",
    "            plt.title('Error Distribution by Radiation Level')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Salva il plot\n",
    "            filename = os.path.join(folder_name, f'solar_radiation_analysis_{timestamp}.png')\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nPlot di analisi salvato come: {filename}\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErrore nel salvare i plot: {str(e)}\")\n",
    "\n",
    "    # Restituisci tutte le metriche in un dizionario\n",
    "    metrics = {\n",
    "        'absolute': {\n",
    "            'mae': mae_raw,\n",
    "            'rmse': rmse_raw,\n",
    "            'r2': r2_raw,\n",
    "            'mape': mape\n",
    "        },\n",
    "        'percentage_accuracy': {\n",
    "            'within_5_percent': within_5_percent,\n",
    "            'within_10_percent': within_10_percent,\n",
    "            'within_20_percent': within_20_percent\n",
    "        },\n",
    "        'categorical': {\n",
    "            'level_accuracy': level_accuracy\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_training_history(history, folder_name=None):\n",
    "    \"\"\"\n",
    "    Visualizza e salva i plot della loss e delle metriche durante il training\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : tensorflow.keras.callbacks.History\n",
    "        L'oggetto history restituito dal training del modello\n",
    "    folder_name : str\n",
    "        Cartella dove salvare il plot\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    try:\n",
    "        # Crea la figura\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Plot della Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot del MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['mae'], label='Training MAE')\n",
    "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Model MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if folder_name is not None:\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            # Genera il nome del file con timestamp\n",
    "            filename = os.path.join(folder_name, 'training_history.png')\n",
    "\n",
    "            # Salva la figura\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nPlot della training history salvato come: {filename}\")\n",
    "\n",
    "        # Salva anche i dati numerici in formato CSV\n",
    "        history_df = pd.DataFrame({\n",
    "            'epoch': range(1, len(history.history['loss']) + 1),\n",
    "            'training_loss': history.history['loss'],\n",
    "            'validation_loss': history.history['val_loss'],\n",
    "            'training_mae': history.history['mae'],\n",
    "            'validation_mae': history.history['val_mae']\n",
    "        })\n",
    "\n",
    "        if folder_name is not None:\n",
    "            csv_filename = os.path.join(folder_name, 'training_history.csv')\n",
    "            history_df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Dati della training history salvati come: {csv_filename}\")\n",
    "\n",
    "        # Calcola e salva le statistiche finali\n",
    "        final_stats = {\n",
    "            'final_training_loss': history.history['loss'][-1],\n",
    "            'final_validation_loss': history.history['val_loss'][-1],\n",
    "            'final_training_mae': history.history['mae'][-1],\n",
    "            'final_validation_mae': history.history['val_mae'][-1],\n",
    "            'best_validation_loss': min(history.history['val_loss']),\n",
    "            'best_validation_mae': min(history.history['val_mae']),\n",
    "            'epochs': len(history.history['loss']),\n",
    "        }\n",
    "\n",
    "        if folder_name is not None:\n",
    "            # Salva le statistiche in formato JSON\n",
    "            stats_filename = os.path.join(folder_name, 'training_stats.json')\n",
    "            with open(stats_filename, 'w') as f:\n",
    "                json.dump(final_stats, f, indent=4)\n",
    "            print(f\"Statistiche finali salvate come: {stats_filename}\")\n",
    "\n",
    "        # Stampa le statistiche principali\n",
    "        print(\"\\nStatistiche finali del training:\")\n",
    "        print(f\"Loss finale (train/val): {final_stats['final_training_loss']:.4f}/{final_stats['final_validation_loss']:.4f}\")\n",
    "        print(f\"MAE finale (train/val): {final_stats['final_training_mae']:.4f}/{final_stats['final_validation_mae']:.4f}\")\n",
    "        print(f\"Miglior validation loss: {final_stats['best_validation_loss']:.4f}\")\n",
    "        print(f\"Miglior validation MAE: {final_stats['best_validation_mae']:.4f}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nErrore durante la creazione o il salvataggio dei plot: {str(e)}\")\n",
    "\n",
    "\n",
    "def train_hybrid_model(model, X_train, y_train, X_test, y_test, epochs=100, batch_size=32, folder_name='solarradiation_index'):\n",
    "    \"\"\"\n",
    "    Funzione di training avanzata per il modello ibrido UV index con monitoraggio dettagliato\n",
    "    e gestione del training.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Il modello ibrido compilato\n",
    "    X_train : numpy.ndarray\n",
    "        Dati di training\n",
    "    y_train : numpy.ndarray\n",
    "        Target di training\n",
    "    X_test : numpy.ndarray\n",
    "        Dati di validation\n",
    "    y_test : numpy.ndarray\n",
    "        Target di validation\n",
    "    epochs : int, optional\n",
    "        Numero massimo di epoche di training\n",
    "    batch_size : int, optional\n",
    "        Dimensione del batch\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    history : keras.callbacks.History\n",
    "        Storia del training con tutte le metriche\n",
    "    \"\"\"\n",
    "\n",
    "    # Callbacks avanzati per il training\n",
    "    callbacks = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            min_delta=1e-4\n",
    "        ),\n",
    "        # ReduceLROnPlateau per MAE\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='mae',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='min',\n",
    "            min_delta=1e-4,\n",
    "            cooldown=3,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        # ReduceLROnPlateau per loss\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            mode='min',\n",
    "            min_delta=1e-4,\n",
    "            cooldown=2,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        # Model Checkpoint\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'{folder_name}_best_model.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            save_weights_only=False\n",
    "        ),\n",
    "        # TensorBoard\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f'./logs_{folder_name}',\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=0\n",
    "        ),\n",
    "        # Lambda Callback per monitoraggio radiazione solare\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: (\n",
    "                lambda y_pred: print(\n",
    "                    f\"\\nEpoch {epoch + 1}:\"\n",
    "                    f\"\\nPredizioni fuori range (0-1500 W/m²): \"\n",
    "                    f\"{np.sum((y_pred < 0) | (y_pred > 1500))}\"\n",
    "                    f\"\\nMAPE: {np.mean(np.abs((y_test - y_pred) / (y_test + 1e-7))) * 100:.2f}%\"\n",
    "                    f\"\\nPredizioni entro ±10%: \"\n",
    "                    f\"{np.mean(np.abs((y_pred - y_test) / (y_test + 1e-7)) <= 0.10) * 100:.2f}%\"\n",
    "                )\n",
    "            )(model.predict(X_test)) if epoch % 20 == 0 else None\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_freq=1,\n",
    "        )\n",
    "\n",
    "        # Analisi post-training\n",
    "        print(\"\\nTraining completato con successo!\")\n",
    "\n",
    "        # Valutazione finale sul test set\n",
    "        test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"\\nMetriche finali sul test set:\")\n",
    "        print(f\"Loss: {test_loss:.4f}\")\n",
    "        print(f\"MAE: {test_mae:.4f}\")\n",
    "        print(f\"MSE: {test_mse:.4f}\")\n",
    "\n",
    "        # Analisi delle predizioni\n",
    "        predictions = model.predict(X_test)\n",
    "        out_of_range = np.sum((predictions < 0) | (predictions > 11))\n",
    "        print(f\"\\nPredizioni fuori range: {out_of_range} ({out_of_range / len(predictions) * 100:.2f}%)\")\n",
    "\n",
    "        plot_training_history(history, folder_name=folder_name)\n",
    "\n",
    "        return history\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nErrore durante il training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Pulizia della memoria\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def calculate_class_weights(y_train, n_classes=12):\n",
    "    \"\"\"\n",
    "    Calcola i pesi delle classi per bilanciare il dataset UV index.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_train : numpy.ndarray\n",
    "        Array dei valori UV di training\n",
    "    n_classes : int, optional\n",
    "        Numero di classi possibili (0-11 per UV index, quindi 12 classi)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict:\n",
    "        Dizionario con i pesi per ogni classe\n",
    "    \"\"\"\n",
    "    # Arrotonda i valori UV al più vicino intero e converti in intero\n",
    "    y_discrete = np.clip(np.round(y_train), 0, 11).astype(int)\n",
    "\n",
    "    # Calcola la frequenza di ogni classe\n",
    "    unique, counts = np.unique(y_discrete, return_counts=True)\n",
    "    total_samples = len(y_discrete)\n",
    "\n",
    "    # Calcola i pesi inversamente proporzionali alla frequenza\n",
    "    weights = {}\n",
    "    for i in range(n_classes):\n",
    "        if i in unique:\n",
    "            # Se la classe è presente, calcola il peso\n",
    "            weight = total_samples / (len(unique) * counts[unique == i][0])\n",
    "        else:\n",
    "            # Se la classe non è presente, assegna un peso neutro\n",
    "            weight = 1.0\n",
    "        weights[i] = weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def integrate_predictions(df, predictions, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Integra le predizioni dell'UV index nel dataset originale per i dati precedenti al 2010.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset originale\n",
    "    predictions : numpy.ndarray\n",
    "        Array delle predizioni UV index\n",
    "    sequence_length : int\n",
    "        Lunghezza della sequenza usata per le predizioni\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataset aggiornato con le predizioni UV index\n",
    "    \"\"\"\n",
    "    # Converti datetime in formato datetime se non lo è già\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # Identifica le righe precedenti al 2010\n",
    "    mask_pre_2010 = df['datetime'].dt.year < 2010\n",
    "\n",
    "    # Crea un DataFrame temporaneo con le predizioni\n",
    "    dates_pre_2010 = df[mask_pre_2010]['datetime'].iloc[sequence_length - 1:]\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'datetime': dates_pre_2010,\n",
    "        'uvindex_predicted': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    # Merge con il dataset originale\n",
    "    df = df.merge(predictions_df, on='datetime', how='left')\n",
    "\n",
    "    # Aggiorna la colonna uvindex dove manca\n",
    "    df['uvindex'] = df['uvindex'].fillna(df['uvindex_predicted'])\n",
    "\n",
    "    # Rimuovi la colonna temporanea\n",
    "    df = df.drop('uvindex_predicted', axis=1)\n",
    "\n",
    "    print(f\"Aggiunte {len(predictions)} predizioni al dataset\")\n",
    "    print(f\"Righe con UV index dopo l'integrazione: {df['uvindex'].notna().sum()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_uvindex_bounded_model(df):\n",
    "    \"\"\"\n",
    "    Training del modello con vincoli specifici per UV index\n",
    "    \"\"\"\n",
    "    print(\"Inizializzazione del training del modello UV index...\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Preparazione dei dati\n",
    "        print(\"\\n1. Preparazione dei dati...\")\n",
    "        X_train_seq, X_test_seq, y_train, y_test, scaler, features, X_to_predict_seq = prepare_hybrid_data(df)\n",
    "\n",
    "        print(f\"Shape dei dati di training: {X_train_seq.shape}\")\n",
    "        print(f\"Shape dei dati di test: {X_test_seq.shape}\")\n",
    "\n",
    "        # Verifica della qualità dei dati\n",
    "        if np.isnan(X_train_seq).any() or np.isnan(y_train).any():\n",
    "            raise ValueError(\"Trovati valori NaN nei dati di training\")\n",
    "\n",
    "        # Creazione del modello\n",
    "        print(\"\\n2. Creazione del modello...\")\n",
    "        input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "        model = create_solarradiation_model(input_shape, folder_name)\n",
    "\n",
    "        print(\"\\n4. Avvio del training...\")\n",
    "        history = train_hybrid_model(\n",
    "            model=model,\n",
    "            X_train=X_train_seq,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_seq,\n",
    "            y_test=y_test,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            folder_name=folder_name\n",
    "        )\n",
    "\n",
    "        print(\"\\n5. Generazione delle predizioni...\")\n",
    "        predictions = model.predict(X_test_seq)\n",
    "        predictions = np.clip(predictions, 0, 11)\n",
    "\n",
    "        print(\"\\n6. Valutazione del modello...\")\n",
    "        metrics = evaluate_solarradiation_predictions(y_test, predictions, folder_name=folder_name)\n",
    "\n",
    "        # Creazione del dizionario dei risultati\n",
    "        training_results = {\n",
    "            'model_params': {\n",
    "                'input_shape': input_shape,\n",
    "                'n_features': len(features),\n",
    "                'sequence_length': X_train_seq.shape[1]\n",
    "            },\n",
    "            'training_params': {\n",
    "                'batch_size': 32,\n",
    "                'total_epochs': len(history.history['loss']),\n",
    "                'best_epoch': np.argmin(history.history['val_loss']) + 1,\n",
    "                #'class_weights': {str(k): float(v) for k, v in class_weights.items()}\n",
    "            },\n",
    "            'performance_metrics': {\n",
    "                'final_loss': float(history.history['val_loss'][-1]),\n",
    "                'final_mae': float(history.history['val_mae'][-1]),\n",
    "                'best_val_loss': float(min(history.history['val_loss'])),\n",
    "                'out_of_range_predictions': int(np.sum((predictions < 0) | (predictions > 11)))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\n7. Predizione dei dati mancanti risultati...\")\n",
    "        to_predict_predictions = model.predict(X_to_predict_seq)\n",
    "        to_predict_predictions = np.clip(to_predict_predictions, 0, 11)\n",
    "\n",
    "        print(\"\\n8. Integrazione delle predizioni nel dataset originale...\")\n",
    "        df_updated = integrate_predictions(df.copy(), to_predict_predictions)\n",
    "\n",
    "        df_updated.to_parquet('./data/weather_data_uvindex.parquet')\n",
    "\n",
    "        # Aggiungi statistiche sulle predizioni al training_results\n",
    "        training_results['prediction_stats'] = {\n",
    "            'n_predictions_added': len(to_predict_predictions),\n",
    "            'mean_predicted_uv': float(to_predict_predictions.mean()),\n",
    "            'min_predicted_uv': float(to_predict_predictions.min()),\n",
    "            'max_predicted_uv': float(to_predict_predictions.max()),\n",
    "        }\n",
    "\n",
    "        print(\"\\nTraining completato con successo!\")\n",
    "\n",
    "        return model, scaler, features, history, predictions, y_test, metrics, training_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nErrore durante il training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Pulizia della memoria\n",
    "        tf.keras.backend.clear_session()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "df = pd.read_parquet('../data/weather_data.parquet')\n",
    "\n",
    "# Esegui il training\n",
    "model, scaler, features, history, predictions, y_test, metrics, training_results = train_uvindex_bounded_model(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "637891db-8d55-4232-a56e-9759dbcc8c2f",
   "metadata": {},
   "source": [
    "def analyze_solarradiation_prediction_quality(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Analisi dettagliata della qualità delle predizioni UV\n",
    "    \"\"\"\n",
    "    # Converti in numpy array e appiattisci\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "\n",
    "    # Arrotonda le predizioni al più vicino 0.5\n",
    "    y_pred_rounded = np.round(y_pred * 2) / 2\n",
    "\n",
    "    # Calcola diverse metriche di accuratezza usando array numpy\n",
    "    exact_match = np.mean(np.abs(y_pred_rounded - y_true) < 1e-6) * 100  # uso di tolleranza per confronti float\n",
    "    within_half = np.mean(np.abs(y_pred_rounded - y_true) <= 0.5) * 100\n",
    "    within_one = np.mean(np.abs(y_pred_rounded - y_true) <= 1.0) * 100\n",
    "\n",
    "    # Analisi per livello di rischio UV\n",
    "    def get_solarradiation_risk_level(values):\n",
    "        # Vettorizzazione della funzione per array numpy\n",
    "        levels = np.zeros_like(values, dtype=str)\n",
    "        levels[values <= 2] = 'Basso'\n",
    "        levels[(values > 2) & (values <= 5)] = 'Moderato'\n",
    "        levels[(values > 5) & (values <= 7)] = 'Alto'\n",
    "        levels[(values > 7) & (values <= 10)] = 'Molto Alto'\n",
    "        levels[values > 10] = 'Estremo'\n",
    "        return levels\n",
    "\n",
    "    y_true_risk = get_solarradiation_risk_level(y_true)\n",
    "    y_pred_risk = get_solarradiation_risk_level(y_pred_rounded)\n",
    "\n",
    "    risk_accuracy = np.mean(y_true_risk == y_pred_risk) * 100\n",
    "\n",
    "    print(\"Analisi Precisione Predizioni UV Index:\")\n",
    "    print(f\"Precisione esatta: {exact_match:.1f}%\")\n",
    "    print(f\"Precisione entro 0.5 punti: {within_half:.1f}%\")\n",
    "    print(f\"Precisione entro 1.0 punti: {within_one:.1f}%\")\n",
    "    print(f\"Precisione livello di rischio: {risk_accuracy:.1f}%\")\n",
    "\n",
    "    # Distribuzione degli errori per livello UV\n",
    "    solarradiation_ranges = [(0, 2), (2, 5), (5, 7), (7, 10), (10, 11)]\n",
    "    labels = ['Basso', 'Moderato', 'Alto', 'Molto Alto', 'Estremo']\n",
    "\n",
    "    print(\"\\nAnalisi errori per livello UV:\")\n",
    "    for (low, high), label in zip(solarradiation_ranges, labels):\n",
    "        mask = (y_true >= low) & (y_true < high)\n",
    "        if np.sum(mask) > 0:\n",
    "            mae_range = np.mean(np.abs(y_pred[mask] - y_true[mask]))\n",
    "            n_samples = np.sum(mask)\n",
    "            print(f\"MAE per UV {label} ({low}-{high}): {mae_range:.3f} (n={n_samples})\")\n",
    "\n",
    "    # Analisi aggiuntiva della distribuzione degli errori\n",
    "    errors = y_pred - y_true\n",
    "    print(\"\\nStatistiche degli errori:\")\n",
    "    print(f\"Media errori: {np.mean(errors):.3f}\")\n",
    "    print(f\"Deviazione standard errori: {np.std(errors):.3f}\")\n",
    "    print(f\"Errore mediano: {np.median(errors):.3f}\")\n",
    "    print(f\"95° percentile errore assoluto: {np.percentile(np.abs(errors), 95):.3f}\")\n",
    "\n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'within_half': within_half,\n",
    "        'within_one': within_one,\n",
    "        'risk_accuracy': risk_accuracy,\n",
    "        'error_stats': {\n",
    "            'mean': float(np.mean(errors)),\n",
    "            'std': float(np.std(errors)),\n",
    "            'median': float(np.median(errors)),\n",
    "            'p95_abs': float(np.percentile(np.abs(errors), 95))\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Per utilizzare l'analisi:\n",
    "metrics = analyze_solarradiation_prediction_quality(y_test, predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08fd4208-0afb-4bf1-bdef-b10b4065fe55",
   "metadata": {},
   "source": [
    "def plot_error_analysis(y_true, y_pred, folder_name=None):\n",
    "    \"\"\"\n",
    "    Funzione per visualizzare l'analisi degli errori di predizione\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Valori reali\n",
    "    y_pred : array-like\n",
    "        Valori predetti\n",
    "    folder_name : str, optional\n",
    "        Cartella dove salvare i plot. Se None, i plot non vengono salvati.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Converti in array numpy 1D se necessario\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "        y_pred = y_pred.values\n",
    "\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # Calcola gli errori\n",
    "    errors = y_pred - y_true\n",
    "\n",
    "    # Crea la figura principale\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Distribuzione degli errori\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.title('Distribuzione degli Errori di Predizione')\n",
    "    plt.xlabel('Errore')\n",
    "    plt.ylabel('Frequenza')\n",
    "\n",
    "    # Plot 2: Actual vs Predicted\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.title('Valori Reali vs Predetti')\n",
    "    plt.xlabel('Valori Reali')\n",
    "    plt.ylabel('Valori Predetti')\n",
    "\n",
    "    # Plot 3: Errori vs Valori Reali\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Errori vs Valori Reali')\n",
    "    plt.xlabel('Valori Reali')\n",
    "    plt.ylabel('Errore')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salva il plot se è specificata una cartella\n",
    "    if folder_name is not None:\n",
    "        try:\n",
    "            # Crea la cartella se non esiste\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "            # Genera il nome del file con timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = os.path.join(folder_name, f'error_analysis_{timestamp}.png')\n",
    "\n",
    "            # Salva la figura\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nPlot salvato come: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErrore nel salvare il plot: {str(e)}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Stampa statistiche degli errori\n",
    "    print(\"\\nStatistiche degli errori:\")\n",
    "    print(f\"MAE: {np.mean(np.abs(errors)):.4f}\")\n",
    "    print(f\"MSE: {np.mean(errors ** 2):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(np.mean(errors ** 2)):.4f}\")\n",
    "    print(f\"Media errori: {np.mean(errors):.4f}\")\n",
    "    print(f\"Std errori: {np.std(errors):.4f}\")\n",
    "\n",
    "    # Calcola percentuali di errori entro certe soglie\n",
    "    thresholds = [0.5, 1.0, 1.5, 2.0]\n",
    "    for threshold in thresholds:\n",
    "        within_threshold = np.mean(np.abs(errors) <= threshold) * 100\n",
    "        print(f\"Predizioni entro ±{threshold}: {within_threshold:.1f}%\")\n",
    "\n",
    "\n",
    "plot_error_analysis(y_test, predictions, folder_name=folder_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03bb9564-e518-4662-b3ee-4cfa96cdf696",
   "metadata": {},
   "source": [
    "def plot_advanced_prediction_analysis(y_true, y_pred, folder_name=None):\n",
    "    \"\"\"\n",
    "    Funzione per visualizzare l'analisi degli errori di predizione e la precisione\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Valori reali\n",
    "    y_pred : array-like\n",
    "        Valori predetti\n",
    "    folder_name : str, optional\n",
    "        Cartella dove salvare i plot. Se None, i plot non vengono salvati.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Converti in array numpy 1D se necessario\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "        y_pred = y_pred.values\n",
    "\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # Calcola gli errori\n",
    "    errors = y_pred - y_true\n",
    "\n",
    "    # Calcola accuracy per diversi livelli di tolleranza\n",
    "    exact_accuracy = np.mean(np.abs(errors) < 0.1) * 100\n",
    "    accuracy_05 = np.mean(np.abs(errors) <= 0.5) * 100\n",
    "    accuracy_10 = np.mean(np.abs(errors) <= 1.0) * 100\n",
    "\n",
    "    def get_risk_level(uv):\n",
    "        if uv < 2:\n",
    "            return 'Basso'\n",
    "        elif uv < 5:\n",
    "            return 'Moderato'\n",
    "        elif uv < 7:\n",
    "            return 'Alto'\n",
    "        elif uv < 10:\n",
    "            return 'Molto Alto'\n",
    "        else:\n",
    "            return 'Estremo'\n",
    "\n",
    "    y_true_risk = [get_risk_level(x) for x in y_true]\n",
    "    y_pred_risk = [get_risk_level(x) for x in y_pred]\n",
    "    risk_accuracy = np.mean(np.array(y_true_risk) == np.array(y_pred_risk)) * 100\n",
    "\n",
    "    # Crea la figura principale\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Plot 1: Distribuzione degli errori\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.title('Distribuzione degli Errori di Predizione')\n",
    "    plt.xlabel('Errore')\n",
    "    plt.ylabel('Frequenza')\n",
    "\n",
    "    # Plot 2: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.title('Valori Reali vs Predetti')\n",
    "    plt.xlabel('Valori Reali')\n",
    "    plt.ylabel('Valori Predetti')\n",
    "\n",
    "    # Plot 3: Errori vs Valori Reali\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Errori vs Valori Reali')\n",
    "    plt.xlabel('Valori Reali')\n",
    "    plt.ylabel('Errore')\n",
    "\n",
    "    # Plot 4: Precisione per intervallo di UV\n",
    "    plt.subplot(2, 2, 4)\n",
    "\n",
    "    solarradiation_ranges = [(0, 2), (2, 5), (5, 7), (7, 10), (10, 11)]\n",
    "    range_labels = ['Basso\\n(0-2)', 'Moderato\\n(2-5)', 'Alto\\n(5-7)', 'Molto Alto\\n(7-10)', 'Estremo\\n(10-11)']\n",
    "\n",
    "    accuracies = []\n",
    "    counts = []\n",
    "    mae_per_range = []\n",
    "\n",
    "    for (low, high) in solarradiation_ranges:\n",
    "        mask = (y_true >= low) & (y_true < high)\n",
    "        if mask.any():\n",
    "            mae = np.mean(np.abs(y_pred[mask] - y_true[mask]))\n",
    "            mae_per_range.append(mae)\n",
    "            count = np.sum(mask)\n",
    "            counts.append(count)\n",
    "            accuracy = np.mean(np.abs(y_pred[mask] - y_true[mask]) <= 0.5) * 100\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    # Crea il grafico a barre con doppio asse y\n",
    "    ax = plt.gca()\n",
    "    bars = plt.bar(range_labels, accuracies, alpha=0.6, color='skyblue')\n",
    "    plt.ylabel('Precisione (%)')\n",
    "    plt.title('Precisione e MAE per Range UV')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                 f'{height:.1f}%\\n(n={counts[bars.index(bar)]})',\n",
    "                 ha='center', va='bottom')\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    line = ax2.plot(range_labels, mae_per_range, 'r-', marker='o', label='MAE')\n",
    "    ax2.set_ylabel('MAE', color='red')\n",
    "\n",
    "    for i, mae in enumerate(mae_per_range):\n",
    "        ax2.text(i, mae, f'MAE: {mae:.3f}', color='red', ha='center', va='bottom')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salva la figura principale se è specificata una cartella\n",
    "    if folder_name is not None:\n",
    "        try:\n",
    "            # Crea la cartella se non esiste\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "            # Genera il timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            # Salva la figura principale\n",
    "            main_plot_filename = os.path.join(folder_name, f'advanced_analysis_{timestamp}.png')\n",
    "            plt.savefig(main_plot_filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nPlot principale salvato come: {main_plot_filename}\")\n",
    "\n",
    "            # Crea e salva la matrice di confusione come plot separato\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            cm = confusion_matrix(y_true_risk, y_pred_risk)\n",
    "            risk_levels = ['Basso', 'Moderato', 'Alto', 'Molto Alto', 'Estremo']\n",
    "            cm_df = pd.DataFrame(cm, columns=risk_levels, index=risk_levels)\n",
    "\n",
    "            sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Matrice di Confusione per Livelli di Rischio UV')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            conf_matrix_filename = os.path.join(folder_name, f'confusion_matrix_{timestamp}.png')\n",
    "            plt.savefig(conf_matrix_filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Matrice di confusione salvata come: {conf_matrix_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErrore nel salvare i plot: {str(e)}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Stampa delle statistiche e analisi\n",
    "    cm = confusion_matrix(y_true_risk, y_pred_risk)\n",
    "    risk_levels = ['Basso', 'Moderato', 'Alto', 'Molto Alto', 'Estremo']\n",
    "    cm_df = pd.DataFrame(cm, columns=risk_levels, index=risk_levels)\n",
    "\n",
    "    print(\"\\nMatrice di Confusione per Livelli di Rischio UV:\")\n",
    "    print(cm_df)\n",
    "\n",
    "    print(\"\\nAnalisi Precisione Predizioni UV Index:\")\n",
    "    print(f\"Precisione esatta (±0.1): {exact_accuracy:.1f}%\")\n",
    "    print(f\"Precisione entro 0.5 punti: {accuracy_05:.1f}%\")\n",
    "    print(f\"Precisione entro 1.0 punti: {accuracy_10:.1f}%\")\n",
    "    print(f\"Precisione livello di rischio: {risk_accuracy:.1f}%\")\n",
    "\n",
    "    print(\"\\nAnalisi errori per livello UV:\")\n",
    "    solarradiation_ranges = [(0, 2, 'Basso'), (2, 5, 'Moderato'), (5, 7, 'Alto'),\n",
    "                             (7, 10, 'Molto Alto'), (10, 11, 'Estremo')]\n",
    "\n",
    "    for low, high, label in solarradiation_ranges:\n",
    "        mask = (y_true >= low) & (y_true < high)\n",
    "        if mask.any():\n",
    "            mae = np.mean(np.abs(errors[mask]))\n",
    "            n_samples = np.sum(mask)\n",
    "            print(f\"MAE per UV {label} ({low}-{high}): {mae:.3f} (n={n_samples})\")\n",
    "\n",
    "    print(\"\\nStatistiche degli errori:\")\n",
    "    print(f\"Media errori: {np.mean(errors):.3f}\")\n",
    "    print(f\"Deviazione standard errori: {np.std(errors):.3f}\")\n",
    "    print(f\"Errore mediano: {np.median(errors):.3f}\")\n",
    "    print(f\"95° percentile errore assoluto: {np.percentile(np.abs(errors), 95):.3f}\")\n",
    "\n",
    "    print(\"\\nDistribuzione degli errori:\")\n",
    "    thresholds = [0.5, 1.0, 1.5, 2.0]\n",
    "    for threshold in thresholds:\n",
    "        within_threshold = np.mean(np.abs(errors) <= threshold) * 100\n",
    "        print(f\"Predizioni entro ±{threshold}: {within_threshold:.1f}%\")\n",
    "\n",
    "\n",
    "# Usa la funzione\n",
    "plot_advanced_prediction_analysis(y_test, predictions, folder_name=folder_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe898941-2338-4157-b624-680bc2c517d8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
